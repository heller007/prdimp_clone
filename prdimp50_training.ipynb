{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrDiMP50 Training and Evaluation on GOT-10k\n",
    "\n",
    "This notebook implements PrDiMP50 (klcedimpnet50) with ResNet50 backbone, trains it on GOT-10k dataset, and evaluates the results.\n",
    "\n",
    "## ⚠️ Important: NumPy Compatibility\n",
    "\n",
    "**If you encounter NumPy 2.x compatibility errors**, the notebook will automatically fix this by downgrading to NumPy <2.0. You may need to restart the kernel/runtime after the first cell runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Fix NumPy 2.x compatibility issue first\n",
    "# NumPy 2.x causes import errors with matplotlib and other packages\n",
    "# This must run BEFORE any other imports\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def fix_numpy_version():\n",
    "    \"\"\"Ensure NumPy <2.0 is installed for compatibility\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        numpy_version = np.__version__\n",
    "        print(f\"Detected NumPy version: {numpy_version}\")\n",
    "        \n",
    "        if numpy_version.startswith('2.'):\n",
    "            print(\"\\n⚠️  NumPy 2.x detected - this causes compatibility issues!\")\n",
    "            print(\"Downgrading to NumPy <2.0...\")\n",
    "            subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                \"--upgrade\", \"--force-reinstall\", \"numpy<2.0\", \"--no-deps\"\n",
    "            ], check=True)\n",
    "            print(\"✅ NumPy downgraded. Please RESTART the kernel/runtime now!\")\n",
    "            print(\"   (In Colab: Runtime > Restart runtime)\")\n",
    "            print(\"   (In Kaggle: Click 'Restart' button)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✅ NumPy version {numpy_version} is compatible\")\n",
    "            return False\n",
    "    except ImportError:\n",
    "        print(\"Installing NumPy <2.0...\")\n",
    "        subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2.0\"\n",
    "        ], check=True)\n",
    "        return False\n",
    "\n",
    "needs_restart = fix_numpy_version()\n",
    "if needs_restart:\n",
    "    raise RuntimeError(\n",
    "        \"NumPy was downgraded. Please RESTART the kernel/runtime and run this cell again.\\n\"\n",
    "        \"In Colab: Runtime > Restart runtime\\n\"\n",
    "        \"In Kaggle: Click the 'Restart' button\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in Kaggle\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "\n",
    "if KAGGLE:\n",
    "    # Kaggle paths\n",
    "    BASE_DIR = Path('/kaggle/working')\n",
    "    DATA_DIR = Path('/kaggle/input')\n",
    "else:\n",
    "    # Local paths\n",
    "    BASE_DIR = Path.cwd()\n",
    "    DATA_DIR = BASE_DIR / 'data'\n",
    "\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "REPO_URL = \"https://github.com/visionml/pytracking.git\"\n",
    "REPO_DIR = BASE_DIR / \"pytracking\"\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print(\"Cloning pytracking repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "    \n",
    "    # Initialize submodules\n",
    "    print(\"Initializing submodules...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run([\"git\", \"submodule\", \"update\", \"--init\"], check=True)\n",
    "    os.chdir(BASE_DIR)\n",
    "else:\n",
    "    print(\"Repository already exists, skipping clone...\")\n",
    "\n",
    "print(f\"Repository cloned to: {REPO_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to Python path\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Kaggle, most are pre-installed)\n",
    "# IMPORTANT: Fix NumPy compatibility issue - ensure NumPy <2.0 for compatibility\n",
    "import importlib\n",
    "\n",
    "# First, check and fix NumPy version if needed\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_version = np.__version__\n",
    "    print(f\"Current NumPy version: {numpy_version}\")\n",
    "    \n",
    "    # Check if NumPy 2.x is installed (causes compatibility issues with matplotlib)\n",
    "    if numpy_version.startswith('2.'):\n",
    "        print(\"NumPy 2.x detected - downgrading to NumPy <2.0 for compatibility...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"numpy<2.0\"], check=True)\n",
    "        print(\"NumPy downgraded successfully. Please restart the kernel/runtime after this cell.\")\n",
    "        import importlib\n",
    "        importlib.reload(importlib.import_module('numpy'))\n",
    "except ImportError:\n",
    "    print(\"NumPy not found, installing NumPy <2.0...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2.0\"], check=True)\n",
    "\n",
    "required_packages = [\n",
    "    'torch', 'torchvision', 'opencv-python', 'Pillow',\n",
    "    'matplotlib', 'pandas', 'scipy', 'tqdm'\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        missing_packages.append(pkg)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"Installing missing packages: {missing_packages}\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + missing_packages)\n",
    "else:\n",
    "    print(\"All required packages are available\")\n",
    "\n",
    "# Verify NumPy version one more time\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"Final NumPy version: {np.__version__}\")\n",
    "    if np.__version__.startswith('2.'):\n",
    "        print(\"WARNING: NumPy 2.x still detected. You may need to restart the kernel.\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local.py for environment settings (both ltr and pytracking)\n",
    "LTR_LOCAL_PY_PATH = REPO_DIR / \"ltr\" / \"admin\" / \"local.py\"\n",
    "PYTRACKING_LOCAL_PY_PATH = REPO_DIR / \"pytracking\" / \"evaluation\" / \"local.py\"\n",
    "\n",
    "# Set paths - user will provide GOT-10k path\n",
    "# For Kaggle, you can set this as an environment variable\n",
    "GOT10K_PATH = os.environ.get('GOT10K_PATH', '')\n",
    "if not GOT10K_PATH:\n",
    "    # Try default locations\n",
    "    if KAGGLE:\n",
    "        # Try common Kaggle input locations\n",
    "        possible_paths = [\n",
    "            DATA_DIR / \"got10k\" / \"train\",\n",
    "            DATA_DIR / \"got-10k\" / \"train\",\n",
    "            Path(\"/kaggle/input/got10k/train\"),\n",
    "        ]\n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                GOT10K_PATH = str(path)\n",
    "                break\n",
    "        if not GOT10K_PATH:\n",
    "            GOT10K_PATH = str(DATA_DIR / \"got10k\" / \"train\")\n",
    "            print(f\"Warning: GOT-10k path not found. Using default: {GOT10K_PATH}\")\n",
    "            print(\"Please set GOT10K_PATH environment variable or update the path in this cell\")\n",
    "    else:\n",
    "        # Local default\n",
    "        GOT10K_PATH = str(BASE_DIR / \"data\" / \"got10k\" / \"train\")\n",
    "        print(f\"Using default path: {GOT10K_PATH}\")\n",
    "        print(\"To change this, set GOT10K_PATH environment variable or modify this cell\")\n",
    "\n",
    "WORKSPACE_DIR = str(BASE_DIR / \"workspace\")\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "\n",
    "# LTR local.py\n",
    "ltr_local_py_content = f\"\"\"\n",
    "class EnvironmentSettings:\n",
    "    def __init__(self):\n",
    "        self.workspace_dir = '{WORKSPACE_DIR}'\n",
    "        self.tensorboard_dir = self.workspace_dir + '/tensorboard/'\n",
    "        self.pretrained_networks = self.workspace_dir + '/pretrained_networks/'\n",
    "        self.pregenerated_masks = ''\n",
    "        self.lasot_dir = ''\n",
    "        self.got10k_dir = '{GOT10K_PATH}'\n",
    "        self.trackingnet_dir = ''\n",
    "        self.coco_dir = ''\n",
    "        self.lvis_dir = ''\n",
    "        self.sbd_dir = ''\n",
    "        self.imagenet_dir = ''\n",
    "        self.imagenetdet_dir = ''\n",
    "        self.ecssd_dir = ''\n",
    "        self.hkuis_dir = ''\n",
    "        self.msra10k_dir = ''\n",
    "        self.davis_dir = ''\n",
    "        self.youtubevos_dir = ''\n",
    "        self.lasot_candidate_matching_dataset_path = ''\n",
    "\"\"\"\n",
    "\n",
    "with open(LTR_LOCAL_PY_PATH, 'w') as f:\n",
    "    f.write(ltr_local_py_content)\n",
    "\n",
    "# PyTracking evaluation local.py\n",
    "pytracking_local_py_content = f\"\"\"\n",
    "from pytracking.evaluation.environment import EnvSettings\n",
    "\n",
    "def local_env_settings():\n",
    "    settings = EnvSettings()\n",
    "    \n",
    "    # Set paths\n",
    "    settings.got10k_path = '{GOT10K_PATH}'\n",
    "    settings.results_path = '{WORKSPACE_DIR}/tracking_results/'\n",
    "    settings.segmentation_path = '{WORKSPACE_DIR}/segmentation_results/'\n",
    "    settings.network_path = '{WORKSPACE_DIR}/networks/'\n",
    "    settings.result_plot_path = '{WORKSPACE_DIR}/result_plots/'\n",
    "    settings.dataspec_path = '{str(REPO_DIR / \"ltr\" / \"data_specs\")}'\n",
    "    \n",
    "    return settings\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(PYTRACKING_LOCAL_PY_PATH.parent, exist_ok=True)\n",
    "with open(PYTRACKING_LOCAL_PY_PATH, 'w') as f:\n",
    "    f.write(pytracking_local_py_content)\n",
    "\n",
    "print(f\"Environment configured:\")\n",
    "print(f\"  GOT-10k path: {GOT10K_PATH}\")\n",
    "print(f\"  Workspace: {WORKSPACE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Fix PreciseRoI Pooling Extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix PreciseRoI Pooling compilation issue\n",
    "# This extension is required for PrDiMP training\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def fix_prroi_pooling():\n",
    "    \"\"\"Clean cache and force rebuild of PreciseRoI Pooling extension\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Fixing PreciseRoI Pooling extension...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set environment variable to force rebuild\n",
    "    os.environ['TORCH_EXTENSIONS_DIR'] = os.path.expanduser('~/.cache/torch_extensions')\n",
    "    # Force PyTorch to rebuild by setting this\n",
    "    os.environ['FORCE_CUDA'] = '1'\n",
    "    \n",
    "    # Clean PyTorch extension cache COMPLETELY to force rebuild\n",
    "    cache_dir = os.path.expanduser('~/.cache/torch_extensions')\n",
    "    \n",
    "    # Remove the entire cache directory, not just the specific extension\n",
    "    if os.path.exists(cache_dir):\n",
    "        print(f\"Removing entire PyTorch extension cache: {cache_dir}\")\n",
    "        try:\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(\"✅ Cache cleaned successfully\")\n",
    "            # Give it a moment to ensure deletion is complete\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Warning: Could not clean cache: {e}\")\n",
    "            # Try to remove just the prroi_pooling cache\n",
    "            prroi_cache = os.path.join(cache_dir, 'py311_cu124', '_prroi_pooling')\n",
    "            if os.path.exists(prroi_cache):\n",
    "                try:\n",
    "                    shutil.rmtree(prroi_cache)\n",
    "                    print(\"✅ Removed prroi_pooling cache specifically\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Install ninja if needed (required for compilation)\n",
    "    try:\n",
    "        import ninja\n",
    "        print(\"✅ ninja is available\")\n",
    "    except ImportError:\n",
    "        print(\"Installing ninja (required for compilation)...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"ninja\", \"-q\"], check=False)\n",
    "        print(\"✅ ninja installed\")\n",
    "    \n",
    "    # Force compilation by importing functional and actually using it\n",
    "    print(\"\\nAttempting to compile PreciseRoI Pooling extension...\")\n",
    "    print(\"This may take 1-2 minutes on first run...\")\n",
    "    \n",
    "    try:\n",
    "        # Find the source files to potentially touch them\n",
    "        try:\n",
    "            prroi_src_path = os.path.join(REPO_DIR, 'ltr', 'external', 'PreciseRoIPooling', 'pytorch', 'prroi_pool', 'src')\n",
    "            if os.path.exists(prroi_src_path):\n",
    "                # Touch source files to force rebuild detection\n",
    "                for root, dirs, files in os.walk(prroi_src_path):\n",
    "                    for file in files:\n",
    "                        if file.endswith(('.cpp', '.cu', '.cuh')):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            try:\n",
    "                                # Touch the file to update modification time\n",
    "                                os.utime(file_path, None)\n",
    "                            except:\n",
    "                                pass\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Import functional module which handles compilation\n",
    "        from ltr.external.PreciseRoIPooling.pytorch.prroi_pool import functional as prroi_func\n",
    "        \n",
    "        # CRITICAL: Reset the cached module to force recompilation\n",
    "        if hasattr(prroi_func, '_prroi_pooling'):\n",
    "            prroi_func._prroi_pooling = None\n",
    "        \n",
    "        # Also clear any module-level cache\n",
    "        import importlib\n",
    "        modules_to_clear = [\n",
    "            'ltr.external.PreciseRoIPooling.pytorch.prroi_pool.functional',\n",
    "            'ltr.external.PreciseRoIPooling.pytorch.prroi_pool.prroi_pool'\n",
    "        ]\n",
    "        for mod_name in modules_to_clear:\n",
    "            if mod_name in sys.modules:\n",
    "                del sys.modules[mod_name]\n",
    "        \n",
    "        # Now import again to trigger fresh compilation\n",
    "        from ltr.external.PreciseRoIPooling.pytorch.prroi_pool import functional as prroi_func_new\n",
    "        \n",
    "        # Actually call the function to trigger compilation\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Testing compilation with CUDA tensors...\")\n",
    "            dummy_feat = torch.randn(1, 64, 10, 10, device='cuda')\n",
    "            dummy_roi = torch.tensor([[0, 0, 0, 5, 5]], dtype=torch.float32, device='cuda')\n",
    "            \n",
    "            # This will trigger actual compilation - it may take a minute\n",
    "            print(\"   Calling prroi_pool2d (this triggers compilation)...\")\n",
    "            result = prroi_func_new.prroi_pool2d(dummy_feat, dummy_roi, 2, 2, 1.0)\n",
    "            print(\"   ✅ Function call succeeded!\")\n",
    "            \n",
    "            # Verify the .so file was created - check multiple possible locations\n",
    "            cache_dir_check = os.path.expanduser('~/.cache/torch_extensions')\n",
    "            possible_locations = [\n",
    "                os.path.join(cache_dir_check, 'py311_cu124', '_prroi_pooling', '_prroi_pooling.so'),\n",
    "                os.path.join(cache_dir_check, f'py{sys.version_info.major}{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}', '_prroi_pooling', '_prroi_pooling.so'),\n",
    "            ]\n",
    "            \n",
    "            # Also search for any .so file in the cache\n",
    "            so_file = None\n",
    "            for loc in possible_locations:\n",
    "                if os.path.exists(loc):\n",
    "                    so_file = loc\n",
    "                    break\n",
    "            \n",
    "            # If not found, search the cache directory\n",
    "            if so_file is None and os.path.exists(cache_dir_check):\n",
    "                for root, dirs, files in os.walk(cache_dir_check):\n",
    "                    for file in files:\n",
    "                        if file == '_prroi_pooling.so':\n",
    "                            so_file = os.path.join(root, file)\n",
    "                            break\n",
    "                    if so_file:\n",
    "                        break\n",
    "            \n",
    "            if so_file and os.path.exists(so_file):\n",
    "                print(f\"✅ PreciseRoI Pooling extension compiled and verified!\")\n",
    "                print(f\"   Extension file: {so_file}\")\n",
    "                print(f\"   File size: {os.path.getsize(so_file) / 1024 / 1024:.2f} MB\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"⚠️  Extension function worked but .so file location unclear\")\n",
    "                print(\"   This is okay - the extension is loaded in memory\")\n",
    "                print(\"   Training should work now. If it fails, restart kernel and try again.\")\n",
    "                return True\n",
    "        else:\n",
    "            print(\"⚠️  CUDA not available - extension requires CUDA\")\n",
    "            print(\"   Training will fail without CUDA. Please enable GPU.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to compile PreciseRoI Pooling: {e}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Ensure CUDA is available: torch.cuda.is_available()\")\n",
    "        print(\"2. Check CUDA version: torch.version.cuda\")\n",
    "        print(\"3. Verify PyTorch CUDA compatibility\")\n",
    "        print(\"4. Try restarting the kernel and running this cell again\")\n",
    "        print(\"5. On Kaggle, ensure GPU accelerator is enabled\")\n",
    "        return False\n",
    "\n",
    "# Fix the extension\n",
    "print(\"CUDA Status:\")\n",
    "print(f\"  Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "success = fix_prroi_pooling()\n",
    "if not success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"⚠️  WARNING: PreciseRoI Pooling extension setup incomplete.\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training will attempt to compile it on first use.\")\n",
    "    print(\"If training fails with the same error:\")\n",
    "    print(\"  1. Restart the kernel\")\n",
    "    print(\"  2. Run all cells from the beginning\")\n",
    "    print(\"  3. Ensure GPU is enabled (Kaggle: Settings > Accelerator > GPU)\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ PreciseRoI Pooling extension is ready!\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Import Required Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTracking imports\n",
    "from ltr.dataset import Got10k\n",
    "from ltr.data import processing, sampler, LTRLoader\n",
    "from ltr.models.tracking import dimpnet\n",
    "import ltr.models.loss as ltr_losses\n",
    "import ltr.models.loss.kl_regression as klreg_losses\n",
    "import ltr.actors.tracking as tracking_actors\n",
    "from ltr.trainers import LTRTrainer\n",
    "import ltr.data.transforms as tfm\n",
    "from ltr.admin.environment import env_settings\n",
    "\n",
    "# Evaluation imports\n",
    "from pytracking.evaluation.datasets import get_dataset\n",
    "from pytracking.evaluation.tracker import Tracker\n",
    "from pytracking.evaluation.running import run_dataset\n",
    "from pytracking.analysis.plot_results import plot_results, print_results\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Visualize Dataset Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GOT-10k dataset for training\n",
    "settings_env = env_settings()\n",
    "got10k_train = Got10k(root=settings_env.got10k_dir, split='vottrain')\n",
    "\n",
    "print(f\"GOT-10k training sequences: {len(got10k_train.sequence_list)}\")\n",
    "print(f\"Sample sequences: {got10k_train.sequence_list[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=4, seq_ids=None):\n",
    "    \"\"\"Visualize dataset samples with bounding boxes\"\"\"\n",
    "    if seq_ids is None:\n",
    "        seq_ids = np.random.choice(len(dataset.sequence_list), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, seq_id in enumerate(seq_ids):\n",
    "        # Get sequence info\n",
    "        seq_info = dataset.get_sequence_info(seq_id)\n",
    "        \n",
    "        # Get first frame\n",
    "        frame_ids = [0]\n",
    "        frames, anno, meta = dataset.get_frames(seq_id, frame_ids)\n",
    "        \n",
    "        # Get image and bounding box\n",
    "        img = np.array(frames[0])\n",
    "        bbox = anno['bbox'][0].numpy()  # [x, y, w, h]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        img_with_bbox = img.copy()\n",
    "        x, y, w, h = bbox.astype(int)\n",
    "        cv2.rectangle(img_with_bbox, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "        \n",
    "        # Add text\n",
    "        seq_name = dataset.sequence_list[seq_id]\n",
    "        class_name = dataset.get_class_name(seq_id)\n",
    "        text = f\"{seq_name}\\n{class_name}\"\n",
    "        cv2.putText(img_with_bbox, text, (x, max(y-10, 20)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img_with_bbox)\n",
    "        axes[idx].set_title(f\"Sequence {seq_id}: {seq_name}\", fontsize=12)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        print(f\"Sequence {seq_id} ({seq_name}):\")\n",
    "        print(f\"  Class: {class_name}\")\n",
    "        print(f\"  BBox: [{x}, {y}, {w}, {h}]\")\n",
    "        print(f\"  Image size: {img.shape}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize 4 random samples\n",
    "print(\"Visualizing dataset samples...\")\n",
    "visualize_samples(got10k_train, num_samples=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create PrDiMP50 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PrDiMP50 model (klcedimpnet50) with ResNet50 backbone\n",
    "print(\"Creating PrDiMP50 model...\")\n",
    "\n",
    "# Model parameters (matching prdimp50 training settings)\n",
    "filter_size = 4\n",
    "output_sigma_factor = 1/4\n",
    "feature_sz = 18\n",
    "output_sz = feature_sz * 16\n",
    "output_sigma = output_sigma_factor / 5.0  # search_area_factor = 5.0\n",
    "\n",
    "net = dimpnet.klcedimpnet50(\n",
    "    filter_size=filter_size,\n",
    "    backbone_pretrained=True,  # Use pretrained ResNet50\n",
    "    optim_iter=5,\n",
    "    clf_feat_norm=True,\n",
    "    clf_feat_blocks=0,\n",
    "    final_conv=True,\n",
    "    out_feature_dim=512,\n",
    "    optim_init_step=1.0,\n",
    "    optim_init_reg=0.05,\n",
    "    optim_min_reg=0.05,\n",
    "    gauss_sigma=output_sigma * feature_sz,\n",
    "    alpha_eps=0.05,\n",
    "    normalize_label=True,\n",
    "    init_initializer='zero'\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "class TrainingSettings:\n",
    "    def __init__(self):\n",
    "        self.description = 'PrDiMP50 training on GOT-10k'\n",
    "        self.batch_size = 8 if torch.cuda.is_available() else 2  # Adjust for Kaggle GPU\n",
    "        self.num_workers = 4 if KAGGLE else 8\n",
    "        self.multi_gpu = False\n",
    "        self.print_interval = 200\n",
    "        self.normalize_mean = [0.485, 0.456, 0.406]\n",
    "        self.normalize_std = [0.229, 0.224, 0.225]\n",
    "        self.search_area_factor = 5.0\n",
    "        self.output_sigma_factor = 1/4\n",
    "        self.target_filter_sz = 4\n",
    "        self.feature_sz = 18\n",
    "        self.output_sz = self.feature_sz * 16\n",
    "        self.center_jitter_factor = {'train': 3, 'test': 4.5}\n",
    "        self.scale_jitter_factor = {'train': 0.25, 'test': 0.5}\n",
    "        self.hinge_threshold = 0.05\n",
    "        self.print_stats = ['Loss/total', 'Loss/bb_ce', 'ClfTrain/clf_ce']\n",
    "        \n",
    "        # Project path for saving checkpoints\n",
    "        self.module_name = 'dimp'\n",
    "        self.script_name = 'prdimp50_got10k'\n",
    "        self.project_path = f'ltr/{self.module_name}/{self.script_name}'\n",
    "        \n",
    "        # Training epochs\n",
    "        self.num_epochs = 50\n",
    "\n",
    "settings = TrainingSettings()\n",
    "print(f\"Training settings configured:\")\n",
    "print(f\"  Batch size: {settings.batch_size}\")\n",
    "print(f\"  Workers: {settings.num_workers}\")\n",
    "print(f\"  Epochs: {settings.num_epochs}\")\n",
    "print(f\"  Log interval (batches): {settings.print_interval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform_joint = tfm.Transform(tfm.ToGrayscale(probability=0.05))\n",
    "\n",
    "transform_train = tfm.Transform(\n",
    "    tfm.ToTensorAndJitter(0.2),\n",
    "    tfm.Normalize(mean=settings.normalize_mean, std=settings.normalize_std)\n",
    ")\n",
    "\n",
    "transform_val = tfm.Transform(\n",
    "    tfm.ToTensor(),\n",
    "    tfm.Normalize(mean=settings.normalize_mean, std=settings.normalize_std)\n",
    ")\n",
    "\n",
    "# Data processing\n",
    "output_sigma = settings.output_sigma_factor / settings.search_area_factor\n",
    "proposal_params = {\n",
    "    'boxes_per_frame': 128,\n",
    "    'gt_sigma': (0.05, 0.05),\n",
    "    'proposal_sigma': [(0.05, 0.05), (0.5, 0.5)]\n",
    "}\n",
    "label_params = {\n",
    "    'feature_sz': settings.feature_sz,\n",
    "    'sigma_factor': output_sigma,\n",
    "    'kernel_sz': settings.target_filter_sz\n",
    "}\n",
    "label_density_params = {\n",
    "    'feature_sz': settings.feature_sz,\n",
    "    'sigma_factor': output_sigma,\n",
    "    'kernel_sz': settings.target_filter_sz,\n",
    "    'normalize': True\n",
    "}\n",
    "\n",
    "data_processing_train = processing.KLDiMPProcessing(\n",
    "    search_area_factor=settings.search_area_factor,\n",
    "    output_sz=settings.output_sz,\n",
    "    center_jitter_factor=settings.center_jitter_factor,\n",
    "    scale_jitter_factor=settings.scale_jitter_factor,\n",
    "    mode='sequence',\n",
    "    proposal_params=proposal_params,\n",
    "    label_function_params=label_params,\n",
    "    label_density_params=label_density_params,\n",
    "    transform=transform_train,\n",
    "    joint_transform=transform_joint\n",
    ")\n",
    "\n",
    "data_processing_val = processing.KLDiMPProcessing(\n",
    "    search_area_factor=settings.search_area_factor,\n",
    "    output_sz=settings.output_sz,\n",
    "    center_jitter_factor=settings.center_jitter_factor,\n",
    "    scale_jitter_factor=settings.scale_jitter_factor,\n",
    "    mode='sequence',\n",
    "    proposal_params=proposal_params,\n",
    "    label_function_params=label_params,\n",
    "    label_density_params=label_density_params,\n",
    "    transform=transform_val,\n",
    "    joint_transform=transform_joint\n",
    ")\n",
    "\n",
    "print(\"Data processing configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "env_settings_obj = env_settings()\n",
    "\n",
    "# Training dataset - only GOT-10k\n",
    "got10k_train_dataset = Got10k(root=env_settings_obj.got10k_dir, split='vottrain')\n",
    "\n",
    "# Validation dataset\n",
    "got10k_val = Got10k(root=env_settings_obj.got10k_dir, split='votval')\n",
    "\n",
    "# Train sampler\n",
    "dataset_train = sampler.DiMPSampler(\n",
    "    [got10k_train_dataset],\n",
    "    [1],  # Only GOT-10k\n",
    "    samples_per_epoch=26000,\n",
    "    max_gap=200,\n",
    "    num_test_frames=3,\n",
    "    num_train_frames=3,\n",
    "    processing=data_processing_train\n",
    ")\n",
    "\n",
    "loader_train = LTRLoader(\n",
    "    'train',\n",
    "    dataset_train,\n",
    "    training=True,\n",
    "    batch_size=settings.batch_size,\n",
    "    num_workers=settings.num_workers,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    stack_dim=1\n",
    ")\n",
    "\n",
    "# Validation sampler\n",
    "dataset_val = sampler.DiMPSampler(\n",
    "    [got10k_val],\n",
    "    [1],\n",
    "    samples_per_epoch=5000,\n",
    "    max_gap=200,\n",
    "    num_test_frames=3,\n",
    "    num_train_frames=3,\n",
    "    processing=data_processing_val\n",
    ")\n",
    "\n",
    "loader_val = LTRLoader(\n",
    "    'val',\n",
    "    dataset_val,\n",
    "    training=False,\n",
    "    batch_size=settings.batch_size,\n",
    "    num_workers=settings.num_workers,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    epoch_interval=5,\n",
    "    stack_dim=1\n",
    ")\n",
    "\n",
    "print(f\"Training samples per epoch: {len(dataset_train)}\")\n",
    "print(f\"Validation samples per epoch: {len(dataset_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actor and optimizer\n",
    "objective = {\n",
    "    'bb_ce': klreg_losses.KLRegression(),\n",
    "    'clf_ce': klreg_losses.KLRegressionGrid()\n",
    "}\n",
    "\n",
    "loss_weight = {\n",
    "    'bb_ce': 0.0025,\n",
    "    'clf_ce': 0.25,\n",
    "    'clf_ce_init': 0.25,\n",
    "    'clf_ce_iter': 1.0\n",
    "}\n",
    "\n",
    "actor = tracking_actors.KLDiMPActor(\n",
    "    net=net,\n",
    "    objective=objective,\n",
    "    loss_weight=loss_weight\n",
    ")\n",
    "\n",
    "# Optimizer with different learning rates for different components\n",
    "optimizer = optim.Adam([\n",
    "    {'params': actor.net.classifier.parameters(), 'lr': 1e-3},\n",
    "    {'params': actor.net.bb_regressor.parameters(), 'lr': 1e-3},\n",
    "    {'params': actor.net.feature_extractor.parameters(), 'lr': 2e-5}\n",
    "], lr=2e-4)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.2)\n",
    "\n",
    "print(\"Actor and optimizer created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "from ltr.admin.settings import Settings\n",
    "\n",
    "train_settings = Settings()\n",
    "train_settings.description = settings.description\n",
    "train_settings.batch_size = settings.batch_size\n",
    "train_settings.num_workers = settings.num_workers\n",
    "train_settings.multi_gpu = settings.multi_gpu\n",
    "train_settings.print_interval = settings.print_interval\n",
    "train_settings.project_path = settings.project_path\n",
    "train_settings.print_stats = settings.print_stats\n",
    "\n",
    "trainer = LTRTrainer(\n",
    "    actor,\n",
    "    [loader_train, loader_val],\n",
    "    optimizer,\n",
    "    train_settings,\n",
    "    lr_scheduler\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Will train for {settings.num_epochs} epochs\")\n",
    "print(f\"Checkpoints will be saved to: {train_settings.project_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train(settings.num_epochs, load_latest=True, fail_safe=True)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on GOT-10k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "checkpoint_path = os.path.join(\n",
    "    env_settings().workspace_dir,\n",
    "    settings.project_path,\n",
    "    'checkpoints',\n",
    "    'checkpoint.pth.tar'\n",
    ")\n",
    "\n",
    "# If checkpoint doesn't exist, try latest\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    checkpoint_path = os.path.join(\n",
    "        env_settings().workspace_dir,\n",
    "        settings.project_path,\n",
    "        'checkpoints',\n",
    "        'checkpoint_latest.pth.tar'\n",
    "    )\n",
    "\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(checkpoint['net'], strict=False)\n",
    "    print(\"Model loaded from checkpoint\")\n",
    "else:\n",
    "    print(\"Warning: Checkpoint not found, using current model state\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation\n",
    "from pytracking.parameter.dimp.prdimp50 import parameters\n",
    "\n",
    "# Create tracker\n",
    "tracker_params = parameters()\n",
    "tracker_params.net = net  # Use our trained model\n",
    "\n",
    "# Load GOT-10k test/val dataset for evaluation\n",
    "got10k_eval_dataset = get_dataset('got10k_ltrval')  # or 'got10k_val' for official val\n",
    "\n",
    "print(f\"Evaluation dataset: {len(got10k_eval_dataset)} sequences\")\n",
    "\n",
    "# Create tracker\n",
    "tracker = Tracker(\n",
    "    'prdimp50',\n",
    "    'prdimp50',\n",
    "    run_id=None,\n",
    "    tracker_params=tracker_params\n",
    ")\n",
    "\n",
    "trackers = [tracker]\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "run_dataset(got10k_eval_dataset, trackers, debug=0, threads=0)\n",
    "\n",
    "print(\"Evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Results and Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "report_name = 'prdimp50_got10k_eval'\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "plot_results(\n",
    "    trackers,\n",
    "    got10k_eval_dataset,\n",
    "    report_name,\n",
    "    merge_results=False,\n",
    "    plot_types=('success', 'prec'),\n",
    "    force_evaluation=False\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print_results(\n",
    "    trackers,\n",
    "    got10k_eval_dataset,\n",
    "    report_name,\n",
    "    merge_results=False,\n",
    "    plot_types=('success', 'prec')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display key metrics\n",
    "from pytracking.analysis.extract_results import extract_results\n",
    "\n",
    "results = extract_results(trackers, got10k_eval_dataset, report_name, force_evaluation=False)\n",
    "\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Success plot metrics\n",
    "    if 'success' in results:\n",
    "        success_data = results['success']\n",
    "        print(f\"\\nSuccess Plot (AUC): {success_data.get('AUC', 'N/A')}\")\n",
    "        print(f\"Success at 0.5 overlap: {success_data.get('OP50', 'N/A')}\")\n",
    "        print(f\"Success at 0.75 overlap: {success_data.get('OP75', 'N/A')}\")\n",
    "    \n",
    "    # Precision metrics\n",
    "    if 'precision' in results:\n",
    "        prec_data = results['precision']\n",
    "        print(f\"\\nPrecision (20px threshold): {prec_data.get('precision_score', 'N/A')}\")\n",
    "    \n",
    "    # Normalized precision\n",
    "    if 'norm_precision' in results:\n",
    "        norm_prec_data = results['norm_precision']\n",
    "        print(f\"\\nNormalized Precision: {norm_prec_data.get('norm_precision_score', 'N/A')}\")\n",
    "else:\n",
    "    print(\"Results extraction failed. Check if evaluation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to file\n",
    "summary_path = os.path.join(env_settings().workspace_dir, 'training_summary.txt')\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"PrDiMP50 Training Summary\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Model: PrDiMP50 (klcedimpnet50) with ResNet50 backbone\\n\")\n",
    "    f.write(f\"Dataset: GOT-10k\\n\")\n",
    "    f.write(f\"Training epochs: {settings.num_epochs}\\n\")\n",
    "    f.write(f\"Batch size: {settings.batch_size}\\n\")\n",
    "    f.write(f\"\\nResults:\\n\")\n",
    "    if results:\n",
    "        f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Cloned the pytracking repository\n",
    "2. ✅ Set up the environment for Kaggle\n",
    "3. ✅ Implemented PrDiMP50 (klcedimpnet50) with ResNet50 backbone\n",
    "4. ✅ Visualized dataset samples with bounding boxes\n",
    "5. ✅ Trained the model on GOT-10k dataset\n",
    "6. ✅ Evaluated the model on GOT-10k\n",
    "7. ✅ Generated plots and metrics\n",
    "\n",
    "The trained model checkpoints are saved in the workspace directory and can be used for inference or further training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
