No matching checkpoint file found

/kaggle/working/prdimp_clone/ltr/data/loader.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = batch[0].storage()._new_shared(numel)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = batch[0].storage()._new_shared(numel)
/kaggle/working/prdimp_clone/ltr/data/loader.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = batch[0].storage()._new_shared(numel)
/kaggle/working/prdimp_clone/ltr/data/loader.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = batch[0].storage()._new_shared(numel)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py311_cu124/_prroi_pooling...
No modifications detected for re-loaded extension module _prroi_pooling, skipping build step...
Loading extension module _prroi_pooling...
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [5971968], which does not match the required output shape [3, 8, 3, 288, 288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [3, 8, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [3, 8, 128, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [3, 8, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)
/kaggle/working/prdimp_clone/ltr/data/loader.py:83: UserWarning: An output with one or more elements was resized since it had shape [8664], which does not match the required output shape [3, 8, 19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)
  return torch.stack(batch, 1, out=out)

Training crashed at epoch 1
Traceback for the error!
Traceback (most recent call last):
  File "/kaggle/working/prdimp_clone/ltr/external/PreciseRoIPooling/pytorch/prroi_pool/functional.py", line 30, in _import_prroi_pooling
    _prroi_pooling = load_extension(
                     ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py", line 1380, in load
    return _jit_compile(
           ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py", line 1823, in _jit_compile
    return _import_module_from_library(name, build_directory, is_python_module)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py", line 2245, in _import_module_from_library
    module = importlib.util.module_from_spec(spec)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 573, in module_from_spec
  File "<frozen importlib._bootstrap_external>", line 1233, in create_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ImportError: /root/.cache/torch_extensions/py311_cu124/_prroi_pooling/_prroi_pooling.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/kaggle/working/prdimp_clone/ltr/trainers/base_trainer.py", line 70, in train
    self.train_epoch()
  File "/kaggle/working/prdimp_clone/ltr/trainers/ltr_trainer.py", line 97, in train_epoch
    self.cycle_dataset(loader)
  File "/kaggle/working/prdimp_clone/ltr/trainers/ltr_trainer.py", line 75, in cycle_dataset
    loss, stats = self.actor(data)
                  ^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/actors/tracking.py", line 93, in __call__
    target_scores, bb_scores = self.net(train_imgs=data['train_images'],
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/models/tracking/dimpnet.py", line 60, in forward
    target_scores = self.classifier(train_feat_clf, test_feat_clf, train_bb, *args, **kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/models/target_classifier/linear_filter.py", line 57, in forward
    filter, filter_iter, losses = self.get_filter(train_feat, train_bb, *args, **kwargs)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/models/target_classifier/linear_filter.py", line 94, in get_filter
    weights = self.filter_initializer(feat, bb)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/models/target_classifier/initializer.py", line 164, in forward
    weights = self.filter_pool(feat, bb)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/models/target_classifier/initializer.py", line 45, in forward
    return self.prroi_pool(feat, roi1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/external/PreciseRoIPooling/pytorch/prroi_pool/prroi_pool.py", line 28, in forward
    return prroi_pool2d(features, rois, self.pooled_height, self.pooled_width, self.spatial_scale)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/external/PreciseRoIPooling/pytorch/prroi_pool/functional.py", line 44, in forward
    _prroi_pooling = _import_prroi_pooling()
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/prdimp_clone/ltr/external/PreciseRoIPooling/pytorch/prroi_pool/functional.py", line 36, in _import_prroi_pooling
    raise ImportError('Can not compile Precise RoI Pooling library.')
ImportError: Can not compile Precise RoI Pooling library.